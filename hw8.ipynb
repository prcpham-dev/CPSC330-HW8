{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw8.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning\n",
    "\n",
    "## Homework 8: Introduction to Computer vision and Time Series\n",
    "\n",
    "**Due date: see the [Calendar](https://htmlpreview.github.io/?https://github.com/UBC-CS/cpsc330/blob/master/docs/calendar.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import sha1\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Instructions\n",
    "rubric={points}\n",
    "\n",
    "You will earn points for following these instructions and successfully submitting your work on Gradescope.  \n",
    "\n",
    "### Group wotk instructions\n",
    "\n",
    "**You may work with a partner on this homework and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 2.\n",
    "  \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "- If you would like to use late tokens for the homework, all group members must have the necessary late tokens available. Please note that the late tokens will be counted for all members of the group.   \n",
    "\n",
    "\n",
    "### General submission instructions\n",
    "\n",
    "- Please **read carefully\n",
    "[Use of Generative AI policy](https://ubc-cs.github.io/cpsc330-2025W1/syllabus.html#use-of-generative-ai-in-the-course)** before starting the homework assignment. \n",
    "- **Run all cells before submitting:** Go to `Kernel -> Restart Kernel and Clear All Outputs`, then select `Run -> Run All Cells`. This ensures your notebook runs cleanly from start to finish without errors.\n",
    "  \n",
    "- **Submit your files on Gradescope.**  \n",
    "   - Upload only your `.ipynb` file **with outputs displayed** and any required output files.\n",
    "     \n",
    "   - Do **not** submit other files from your repository.  \n",
    "   - If you need help, see the [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/).  \n",
    "- **Check that outputs render properly.**  \n",
    "   - Make sure all plots and outputs appear in your submission.\n",
    "     \n",
    "   - If your `.ipynb` file is too large and doesn't render on Gradescope, also upload a PDF or HTML version so the TAs can view your work.  \n",
    "- **Keep execution order clean.**  \n",
    "   - Execution numbers must start at \"1\" and increase in order.\n",
    "     \n",
    "   - Notebooks without visible outputs may not be graded.  \n",
    "   - Out-of-order or missing execution numbers may result in mark deductions.  \n",
    "- **Follow course submission guidelines:** Review the [CPSC 330 homework instructions](https://ubc-cs.github.io/cpsc330-2025W1/docs/homework_instructions.html) for detailed guidance on completing and submitting assignments. \n",
    "   \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Exercise 1: time series prediction\n",
    "\n",
    "In this exercise we'll be looking at a [dataset of avocado prices](https://www.kaggle.com/neuromusic/avocado-prices). You should start by downloading the dataset and storing it under the `data` folder. We will be forcasting average avocado price for the next week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-27</td>\n",
       "      <td>1.33</td>\n",
       "      <td>64236.62</td>\n",
       "      <td>1036.74</td>\n",
       "      <td>54454.85</td>\n",
       "      <td>48.16</td>\n",
       "      <td>8696.87</td>\n",
       "      <td>8603.62</td>\n",
       "      <td>93.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-20</td>\n",
       "      <td>1.35</td>\n",
       "      <td>54876.98</td>\n",
       "      <td>674.28</td>\n",
       "      <td>44638.81</td>\n",
       "      <td>58.33</td>\n",
       "      <td>9505.56</td>\n",
       "      <td>9408.07</td>\n",
       "      <td>97.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-13</td>\n",
       "      <td>0.93</td>\n",
       "      <td>118220.22</td>\n",
       "      <td>794.70</td>\n",
       "      <td>109149.67</td>\n",
       "      <td>130.50</td>\n",
       "      <td>8145.35</td>\n",
       "      <td>8042.21</td>\n",
       "      <td>103.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-06</td>\n",
       "      <td>1.08</td>\n",
       "      <td>78992.15</td>\n",
       "      <td>1132.00</td>\n",
       "      <td>71976.41</td>\n",
       "      <td>72.58</td>\n",
       "      <td>5811.16</td>\n",
       "      <td>5677.40</td>\n",
       "      <td>133.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-11-29</td>\n",
       "      <td>1.28</td>\n",
       "      <td>51039.60</td>\n",
       "      <td>941.48</td>\n",
       "      <td>43838.39</td>\n",
       "      <td>75.78</td>\n",
       "      <td>6183.95</td>\n",
       "      <td>5986.26</td>\n",
       "      <td>197.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  AveragePrice  Total Volume     4046       4225    4770  \\\n",
       "0 2015-12-27          1.33      64236.62  1036.74   54454.85   48.16   \n",
       "1 2015-12-20          1.35      54876.98   674.28   44638.81   58.33   \n",
       "2 2015-12-13          0.93     118220.22   794.70  109149.67  130.50   \n",
       "3 2015-12-06          1.08      78992.15  1132.00   71976.41   72.58   \n",
       "4 2015-11-29          1.28      51039.60   941.48   43838.39   75.78   \n",
       "\n",
       "   Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  region  \n",
       "0     8696.87     8603.62       93.25          0.0  conventional  2015  Albany  \n",
       "1     9505.56     9408.07       97.49          0.0  conventional  2015  Albany  \n",
       "2     8145.35     8042.21      103.14          0.0  conventional  2015  Albany  \n",
       "3     5811.16     5677.40      133.76          0.0  conventional  2015  Albany  \n",
       "4     6183.95     5986.26      197.69          0.0  conventional  2015  Albany  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/avocado.csv\", parse_dates=[\"Date\"], index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18249, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2015-01-04 00:00:00')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Date\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2018-03-25 00:00:00')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Date\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the data ranges from the start of 2015 to March 2018 (~2 years ago), for a total of 3.25 years or so. Let's split the data so that we have a 6 months of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = '20170925'\n",
    "df_train = df[df[\"Date\"] <= split_date]\n",
    "df_test  = df[df[\"Date\"] >  split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_train) + len(df_test) == len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.1 How many time series? \n",
    "rubric={points:4}\n",
    "\n",
    "In the [Rain in Australia](https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package) dataset from lecture demo, we had different measurements for each Location. \n",
    "\n",
    "We want you to consider this for the avocado prices dataset. For which categorical feature(s), if any, do we have separate measurements? Justify your answer by referencing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "Avocado dataframe have different region and type. And after we griup by region and type, there are 108 instances. Hence, we will have 108 time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df.groupby([\"region\", \"type\"]).ngroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.2 Equally spaced measurements? \n",
    "rubric={points:4}\n",
    "\n",
    "In the Rain in Australia dataset, the measurements were generally equally spaced but with some exceptions. How about with this dataset? Justify your answer by referencing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "We can see that the dataset have weekly measurements (7 days intervals) with some having the measurement in 7, 14 or 21 days interval or 1, 2 or 3 weeks. Therefore, the dataset is not equally spaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NaT, Timedelta('7 days 00:00:00'), Timedelta('14 days 00:00:00'), Timedelta('21 days 00:00:00')]\n"
     ]
    }
   ],
   "source": [
    "spaces = []\n",
    "\n",
    "for key, group in df.groupby([\"region\", \"type\"]):\n",
    "    group = group.sort_values(\"Date\")\n",
    "    diff = group[\"Date\"].diff().unique()\n",
    "    spaces.append(diff)\n",
    "\n",
    "unique_spaces = []\n",
    "for space in spaces:\n",
    "    for time in space:\n",
    "        if time not in unique_spaces:\n",
    "            unique_spaces.append(time)\n",
    "\n",
    "print(unique_spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.3 Interpreting regions \n",
    "rubric={points:4}\n",
    "\n",
    "In the Rain in Australia dataset, each location was a different place in Australia. For this dataset, look at the names of the regions. Do you think the regions are also all distinct, or are there overlapping regions? Justify your answer by referencing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "I think these are all different city in America so there are no overlaps regions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Albany' 'Atlanta' 'BaltimoreWashington' 'Boise' 'Boston'\n",
      " 'BuffaloRochester' 'California' 'Charlotte' 'Chicago' 'CincinnatiDayton'\n",
      " 'Columbus' 'DallasFtWorth' 'Denver' 'Detroit' 'GrandRapids' 'GreatLakes'\n",
      " 'HarrisburgScranton' 'HartfordSpringfield' 'Houston' 'Indianapolis'\n",
      " 'Jacksonville' 'LasVegas' 'LosAngeles' 'Louisville' 'MiamiFtLauderdale'\n",
      " 'Midsouth' 'Nashville' 'NewOrleansMobile' 'NewYork' 'Northeast'\n",
      " 'NorthernNewEngland' 'Orlando' 'Philadelphia' 'PhoenixTucson'\n",
      " 'Pittsburgh' 'Plains' 'Portland' 'RaleighGreensboro' 'RichmondNorfolk'\n",
      " 'Roanoke' 'Sacramento' 'SanDiego' 'SanFrancisco' 'Seattle'\n",
      " 'SouthCarolina' 'SouthCentral' 'Southeast' 'Spokane' 'StLouis' 'Syracuse'\n",
      " 'Tampa' 'TotalUS' 'West' 'WestTexNewMexico']\n"
     ]
    }
   ],
   "source": [
    "print(df[\"region\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the entire dataset despite any location-based weirdness uncovered in the previous part.\n",
    "\n",
    "We will be trying to forecast the avocado price. The function below is adapted from the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_lag_feature(\n",
    "    df: pd.DataFrame,\n",
    "    orig_feature: str,\n",
    "    lag: int,\n",
    "    groupby: list[str],\n",
    "    new_feature_name: str | None = None,\n",
    "    clip: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a lagged (or ahead) version of a feature, optionally per group.\n",
    "\n",
    "    Assumes df is already sorted by time within each group and has unique indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The dataset.\n",
    "    orig_feature : str\n",
    "        Name of the column to lag.\n",
    "    lag : int\n",
    "        The lag:\n",
    "          - negative → values from the past (t-1, t-2, ...)\n",
    "          - positive → values from the future (t+1, t+2, ...)\n",
    "    groupby : list of str\n",
    "        Column(s) to group by if df contains multiple time series.\n",
    "    new_feature_name : str, optional\n",
    "        Name of the new column. If None, a name is generated automatically.\n",
    "    clip : bool, default False\n",
    "        If True, drop rows where the new feature is NaN.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A new dataframe with the additional column added.\n",
    "    \"\"\"\n",
    "    if lag == 0:\n",
    "        raise ValueError(\"lag cannot be 0 (no shift). Use the original feature instead.\")\n",
    "\n",
    "    # Default name if not provided\n",
    "    if new_feature_name is None:\n",
    "        if lag < 0:\n",
    "            new_feature_name = f\"{orig_feature}_lag{abs(lag)}\"\n",
    "        else:\n",
    "            new_feature_name = f\"{orig_feature}_ahead{lag}\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Map your convention (negative=past, positive=future) to pandas shift\n",
    "    # pandas: shift(+k) → past, shift(-k) → future\n",
    "    periods = abs(lag) if lag < 0 else -lag\n",
    "\n",
    "    df[new_feature_name] = (\n",
    "        df.groupby(groupby, sort=False)[orig_feature]\n",
    "          .shift(periods)\n",
    "    )\n",
    "\n",
    "    if clip:\n",
    "        df = df.dropna(subset=[new_feature_name])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first sort our dataframe properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>1.06</td>\n",
       "      <td>45147.50</td>\n",
       "      <td>941.38</td>\n",
       "      <td>33196.16</td>\n",
       "      <td>164.14</td>\n",
       "      <td>10845.82</td>\n",
       "      <td>10103.35</td>\n",
       "      <td>742.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>70873.60</td>\n",
       "      <td>1353.90</td>\n",
       "      <td>60017.20</td>\n",
       "      <td>179.32</td>\n",
       "      <td>9323.18</td>\n",
       "      <td>9170.82</td>\n",
       "      <td>152.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18244</th>\n",
       "      <td>2018-02-25</td>\n",
       "      <td>1.57</td>\n",
       "      <td>18421.24</td>\n",
       "      <td>1974.26</td>\n",
       "      <td>2482.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13964.33</td>\n",
       "      <td>13698.27</td>\n",
       "      <td>266.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18245</th>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>1.54</td>\n",
       "      <td>17393.30</td>\n",
       "      <td>1832.24</td>\n",
       "      <td>1905.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13655.49</td>\n",
       "      <td>13401.93</td>\n",
       "      <td>253.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18246</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>1.56</td>\n",
       "      <td>22128.42</td>\n",
       "      <td>2162.67</td>\n",
       "      <td>3194.25</td>\n",
       "      <td>8.93</td>\n",
       "      <td>16762.57</td>\n",
       "      <td>16510.32</td>\n",
       "      <td>252.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18247</th>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>15896.38</td>\n",
       "      <td>2055.35</td>\n",
       "      <td>1499.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12341.48</td>\n",
       "      <td>12114.81</td>\n",
       "      <td>226.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18248</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>1.62</td>\n",
       "      <td>15303.40</td>\n",
       "      <td>2325.30</td>\n",
       "      <td>2171.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10806.44</td>\n",
       "      <td>10569.80</td>\n",
       "      <td>236.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18249 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "0     2015-01-04          1.22      40873.28  2819.50  28287.42   49.90   \n",
       "1     2015-01-11          1.24      41195.08  1002.85  31640.34  127.12   \n",
       "2     2015-01-18          1.17      44511.28   914.14  31540.32  135.77   \n",
       "3     2015-01-25          1.06      45147.50   941.38  33196.16  164.14   \n",
       "4     2015-02-01          0.99      70873.60  1353.90  60017.20  179.32   \n",
       "...          ...           ...           ...      ...       ...     ...   \n",
       "18244 2018-02-25          1.57      18421.24  1974.26   2482.65    0.00   \n",
       "18245 2018-03-04          1.54      17393.30  1832.24   1905.57    0.00   \n",
       "18246 2018-03-11          1.56      22128.42  2162.67   3194.25    8.93   \n",
       "18247 2018-03-18          1.56      15896.38  2055.35   1499.55    0.00   \n",
       "18248 2018-03-25          1.62      15303.40  2325.30   2171.66    0.00   \n",
       "\n",
       "       Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "0         9716.46     9186.93      529.53          0.0  conventional  2015   \n",
       "1         8424.77     8036.04      388.73          0.0  conventional  2015   \n",
       "2        11921.05    11651.09      269.96          0.0  conventional  2015   \n",
       "3        10845.82    10103.35      742.47          0.0  conventional  2015   \n",
       "4         9323.18     9170.82      152.36          0.0  conventional  2015   \n",
       "...           ...         ...         ...          ...           ...   ...   \n",
       "18244    13964.33    13698.27      266.06          0.0       organic  2018   \n",
       "18245    13655.49    13401.93      253.56          0.0       organic  2018   \n",
       "18246    16762.57    16510.32      252.25          0.0       organic  2018   \n",
       "18247    12341.48    12114.81      226.67          0.0       organic  2018   \n",
       "18248    10806.44    10569.80      236.64          0.0       organic  2018   \n",
       "\n",
       "                 region  \n",
       "0                Albany  \n",
       "1                Albany  \n",
       "2                Albany  \n",
       "3                Albany  \n",
       "4                Albany  \n",
       "...                 ...  \n",
       "18244  WestTexNewMexico  \n",
       "18245  WestTexNewMexico  \n",
       "18246  WestTexNewMexico  \n",
       "18247  WestTexNewMexico  \n",
       "18248  WestTexNewMexico  \n",
       "\n",
       "[18249 rows x 13 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sort = df.sort_values(by=[\"region\", \"type\", \"Date\"]).reset_index(drop=True)\n",
    "df_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then call `create_lag_feature`. This creates a new column in the dataset `AveragePriceNextWeek`, which is the following week's `AveragePrice`. We have set `clip=True` which means it will remove rows where the target would be missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "      <th>AveragePriceNextWeek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>1.06</td>\n",
       "      <td>45147.50</td>\n",
       "      <td>941.38</td>\n",
       "      <td>33196.16</td>\n",
       "      <td>164.14</td>\n",
       "      <td>10845.82</td>\n",
       "      <td>10103.35</td>\n",
       "      <td>742.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>70873.60</td>\n",
       "      <td>1353.90</td>\n",
       "      <td>60017.20</td>\n",
       "      <td>179.32</td>\n",
       "      <td>9323.18</td>\n",
       "      <td>9170.82</td>\n",
       "      <td>152.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18243</th>\n",
       "      <td>2018-02-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>17597.12</td>\n",
       "      <td>1892.05</td>\n",
       "      <td>1928.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13776.71</td>\n",
       "      <td>13553.53</td>\n",
       "      <td>223.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18244</th>\n",
       "      <td>2018-02-25</td>\n",
       "      <td>1.57</td>\n",
       "      <td>18421.24</td>\n",
       "      <td>1974.26</td>\n",
       "      <td>2482.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13964.33</td>\n",
       "      <td>13698.27</td>\n",
       "      <td>266.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18245</th>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>1.54</td>\n",
       "      <td>17393.30</td>\n",
       "      <td>1832.24</td>\n",
       "      <td>1905.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13655.49</td>\n",
       "      <td>13401.93</td>\n",
       "      <td>253.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18246</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>1.56</td>\n",
       "      <td>22128.42</td>\n",
       "      <td>2162.67</td>\n",
       "      <td>3194.25</td>\n",
       "      <td>8.93</td>\n",
       "      <td>16762.57</td>\n",
       "      <td>16510.32</td>\n",
       "      <td>252.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18247</th>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>15896.38</td>\n",
       "      <td>2055.35</td>\n",
       "      <td>1499.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12341.48</td>\n",
       "      <td>12114.81</td>\n",
       "      <td>226.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18141 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "0     2015-01-04          1.22      40873.28  2819.50  28287.42   49.90   \n",
       "1     2015-01-11          1.24      41195.08  1002.85  31640.34  127.12   \n",
       "2     2015-01-18          1.17      44511.28   914.14  31540.32  135.77   \n",
       "3     2015-01-25          1.06      45147.50   941.38  33196.16  164.14   \n",
       "4     2015-02-01          0.99      70873.60  1353.90  60017.20  179.32   \n",
       "...          ...           ...           ...      ...       ...     ...   \n",
       "18243 2018-02-18          1.56      17597.12  1892.05   1928.36    0.00   \n",
       "18244 2018-02-25          1.57      18421.24  1974.26   2482.65    0.00   \n",
       "18245 2018-03-04          1.54      17393.30  1832.24   1905.57    0.00   \n",
       "18246 2018-03-11          1.56      22128.42  2162.67   3194.25    8.93   \n",
       "18247 2018-03-18          1.56      15896.38  2055.35   1499.55    0.00   \n",
       "\n",
       "       Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "0         9716.46     9186.93      529.53          0.0  conventional  2015   \n",
       "1         8424.77     8036.04      388.73          0.0  conventional  2015   \n",
       "2        11921.05    11651.09      269.96          0.0  conventional  2015   \n",
       "3        10845.82    10103.35      742.47          0.0  conventional  2015   \n",
       "4         9323.18     9170.82      152.36          0.0  conventional  2015   \n",
       "...           ...         ...         ...          ...           ...   ...   \n",
       "18243    13776.71    13553.53      223.18          0.0       organic  2018   \n",
       "18244    13964.33    13698.27      266.06          0.0       organic  2018   \n",
       "18245    13655.49    13401.93      253.56          0.0       organic  2018   \n",
       "18246    16762.57    16510.32      252.25          0.0       organic  2018   \n",
       "18247    12341.48    12114.81      226.67          0.0       organic  2018   \n",
       "\n",
       "                 region  AveragePriceNextWeek  \n",
       "0                Albany                  1.24  \n",
       "1                Albany                  1.17  \n",
       "2                Albany                  1.06  \n",
       "3                Albany                  0.99  \n",
       "4                Albany                  0.99  \n",
       "...                 ...                   ...  \n",
       "18243  WestTexNewMexico                  1.57  \n",
       "18244  WestTexNewMexico                  1.54  \n",
       "18245  WestTexNewMexico                  1.56  \n",
       "18246  WestTexNewMexico                  1.56  \n",
       "18247  WestTexNewMexico                  1.62  \n",
       "\n",
       "[18141 rows x 14 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hastarget = create_lag_feature(df_sort, \"AveragePrice\", +1, [\"region\", \"type\"], \"AveragePriceNextWeek\", clip=True)\n",
    "df_hastarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to predict `AveragePriceNextWeek`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_hastarget[df_hastarget[\"Date\"] <= split_date]\n",
    "df_test  = df_hastarget[df_hastarget[\"Date\"] >  split_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.4 `AveragePrice` baseline \n",
    "rubric={points}\n",
    "\n",
    "Soon we will want to build some models to forecast the average avocado price a week in advance. Before we start with any ML though, let's try a baseline. Previously we used `DummyClassifier` or `DummyRegressor` as a baseline. This time, we'll do something else as a baseline: we'll assume the price stays the same from this week to next week. So, we'll set our prediction of \"AveragePriceNextWeek\" exactly equal to \"AveragePrice\", assuming no change. That is kind of like saying, \"If it's raining today then I'm guessing it will be raining tomorrow\". This simplistic approach will not get a great score but it's a good starting point for reference. If our model does worse that this, it must not be very good. \n",
    "\n",
    "Using this baseline approach, what $R^2$ do you get on the train and test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "Train r2 is 0.82\n",
    "\n",
    "Test r2 is 0.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8285800937261841"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = df_train[\"AveragePriceNextWeek\"]\n",
    "y_train_pred = df_train[\"AveragePrice\"]\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "train_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7631780188583048"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_test  = df_test[\"AveragePriceNextWeek\"]\n",
    "y_test_pred  = df_test[\"AveragePrice\"]\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "test_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "assert not train_r2 is None, \"Are you using the correct variable name?\"\n",
    "assert not test_r2 is None, \"Are you using the correct variable name?\"\n",
    "assert sha1(str(round(train_r2, 3)).encode('utf8')).hexdigest() == 'b1136fe2a8918904393ab6f40bfb3f38eac5fc39', \"Your training score is not correct. Are you using the right features?\"\n",
    "assert sha1(str(round(test_r2, 3)).encode('utf8')).hexdigest() == 'cc24d9a9b567b491a56b42f7adc582f2eefa5907', \"Your test score is not correct. Are you using the right features?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.5 Forecasting average avocado price\n",
    "rubric={points:10}\n",
    "\n",
    "Now that the baseline is done, let's build some models to forecast the average avocado price a week later. Experiment with a few approachs for encoding the date. Justify the decisions you make. Which approach worked best? Report your test score and briefly discuss your results.\n",
    "\n",
    "Benchmark: you should be able to achieve $R^2$ of at least 0.79 on the test set. I got to 0.80, but not beyond that. Let me know if you do better!\n",
    "\n",
    "Note: because we only have 2 splits here, we need to be a bit wary of overfitting on the test set. Try not to test on it a ridiculous number of times. If you are interested in some proper ways of dealing with this, see for example sklearn's [TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html), which is like cross-validation for time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_validate\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make date into index and convert it into POSIX time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_train.set_index(\"Date\").copy()\n",
    "test_df  = df_test.set_index(\"Date\").copy()\n",
    "\n",
    "train_df[\"posix\"] = train_df.index.astype(\"int64\") // 10**9\n",
    "test_df[\"posix\"]  = test_df.index.astype(\"int64\") // 10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "y_train = train_df.values\n",
    "y_test = test_df.values\n",
    "\n",
    "X_train = train_df.index.astype(\"int64\").values.reshape(-1, 1) // 10 ** 9\n",
    "X_test = test_df.index.astype(\"int64\").values.reshape(-1, 1) // 10 ** 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=[\"AveragePriceNextWeek\"])\n",
    "y_train = train_df[\"AveragePriceNextWeek\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"AveragePriceNextWeek\"])\n",
    "y_test = test_df[\"AveragePriceNextWeek\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing:\n",
    "- OneHotEncoder categorical\n",
    "- Scale numerical\n",
    "- Drop unrelated or unuseful information like how many small bag or big bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "numeric = [\"AveragePrice\", \"year\", \"posix\", \"Total Volume\", \"Total Bags\"]\n",
    "categorical = [\"region\", \"type\"]\n",
    "drop = [\"4046\", \"4225\", \"4770\", \"Small Bags\", \"Large Bags\", \"XLarge Bags\"]\n",
    "    \n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"categorical\", OneHotEncoder(handle_unknown=\"ignore\"), categorical),\n",
    "        (\"num\", StandardScaler(), numeric),\n",
    "        (\"drop\", \"drop\", drop)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Ridge to predict with TimeSeriesSplit evaluation on a variety for alpha options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest CV Test R^2: 0.8242\n",
      "Ridge CV Test R^2: 0.8281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prc.__/Documents/UBC/venv/lib/python3.13/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/prc.__/Documents/UBC/venv/lib/python3.13/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/prc.__/Documents/UBC/venv/lib/python3.13/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/prc.__/Documents/UBC/venv/lib/python3.13/site-packages/sklearn/linear_model/_base.py:280: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/prc.__/Documents/UBC/venv/lib/python3.13/site-packages/sklearn/linear_model/_base.py:280: RuntimeWarning: overflow encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/prc.__/Documents/UBC/venv/lib/python3.13/site-packages/sklearn/linear_model/_base.py:280: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/prc.__/Documents/UBC/venv/lib/python3.13/site-packages/sklearn/linear_model/_base.py:280: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/prc.__/Documents/UBC/venv/lib/python3.13/site-packages/sklearn/linear_model/_base.py:280: RuntimeWarning: overflow encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/prc.__/Documents/UBC/venv/lib/python3.13/site-packages/sklearn/linear_model/_base.py:280: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n"
     ]
    }
   ],
   "source": [
    "tscv = TimeSeriesSplit()\n",
    "\n",
    "rf = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"rf\", RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        random_state=0,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "ridge = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"ridge\", Ridge())\n",
    "])\n",
    "\n",
    "\n",
    "rf_scores = cross_validate(\n",
    "    rf, X_train, y_train,\n",
    "    cv=tscv,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "ridge_scores = cross_validate(\n",
    "    ridge, X_train, y_train,\n",
    "    cv=tscv,\n",
    "    return_train_score=True\n",
    ")\n",
    "    \n",
    "print(f\"Random Forest CV Test R^2: {rf_scores['test_score'].mean():.4f}\")\n",
    "print(f\"Ridge CV Test R^2: {ridge_scores['test_score'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final test preformance with ridge since it is higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Ridge Test R^2: 0.783288434329171\n"
     ]
    }
   ],
   "source": [
    "ridge.fit(X_train, y_train)\n",
    "test_pred = ridge.predict(X_test)\n",
    "\n",
    "print(\"Final Ridge Test R^2:\", r2_score(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Short answer questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.1 Time series\n",
    "\n",
    "rubric={points:6}\n",
    "\n",
    "The following questions pertain to Lecture 20 on time series data:\n",
    "\n",
    "1. Sometimes a time series has missing time points or, worse, time points that are unequally spaced in general. Give an example of a real world situation where the time series data would have unequally spaced time points.\n",
    "2. In class we discussed two approaches to using temporal information: encoding the date as one or more features, and creating lagged versions of features. Which of these (one/other/both/neither) two approaches would struggle with unequally spaced time points? Briefly justify your answer.\n",
    "3. When studying time series modeling, we explored several ways to encode date information as a feature for the citibike dataset. When we used time of day as a numeric feature, the Ridge model was not able to capture the periodic pattern. Why? How did we tackle this problem? Briefly explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "1. Hospital emergency room visits. Patients do not arrive at regular intervals. Sometimes there are many arrivals close together and sometimes hours pass with none. The timestamps are irregular and unequally spaced\n",
    "\n",
    "2. Encoding the date (hour, weekday, year) still works fine even if the time gaps are uneven, because it depends only on the timestamp itself, not the spacing between timestamps\n",
    "\n",
    "3. Ridge Regression assumes linear relationships. Linear models can’t wrap around and cannot learn periodic patterns from a raw number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.2 Computer vision \n",
    "rubric={points:6}\n",
    "\n",
    "The following questions pertain to the lecture on multiclass classification and introduction to computer vision. \n",
    "\n",
    "1. How many parameters (coefficients and intercepts) will `sklearn`’s `LogisticRegression()` model learn for a four-class classification problem, assuming that you have 10 features? Briefly explain your answer.\n",
    "2. In Lecture 19, we briefly discussed how neural networks are sort of like `sklearn`'s pipelines, in the sense that they involve multiple sequential transformations of the data, finally resulting in the prediction. Why was this property useful when it came to transfer learning?\n",
    "3. Imagine that you have a small dataset with ~1000 images containing pictures and names of 50 different Computer Science faculty members from UBC. Your goal is to develop a reasonably accurate multi-class classification model for this task. Describe which model/technique you would use and briefly justify your choice in one to three sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "1. Multinomial logistic regression learns one set of weights and one intercept for each class, so with 4 classes and 10 features, that’s that is 4 * (10 + 1) = 44 parameters\n",
    "\n",
    "2. Neural networks consist of multiple layers that transform the raw image into more abstract features. Therefore, in transfer learning, we can reuse the early layers that already learned, and only replace or fine-tune the final layers for our new task\n",
    "\n",
    "4. I would use transfer learning with a pretrained convolutional neural network and fine-tune only the last classification layer. With only ~1000 images and 50 classes, training from scratch would lead to overfit, whereas transfer learning provides pretrained feature extractors that work well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before submitting your assignment, please make sure you have followed all the instructions in the Submission Instructions section at the top. \n",
    "\n",
    "Here is a quick checklist before submitting: \n",
    "\n",
    "- [ ] Restart kernel, clear outputs, and run all cells from top to bottom.  \n",
    "- [ ] `.ipynb` file runs without errors and contains all outputs.  \n",
    "- [ ] Only `.ipynb` and required output files are uploaded (no extra files).  \n",
    "- [ ] Execution numbers start at **1** and are in order.  \n",
    "- [ ] If `.ipynb` is too large and doesn't render on Gradescope, also upload a PDF/HTML version.  \n",
    "- [ ] Reviewed the [CPSC 330 homework instructions](https://ubc-cs.github.io/cpsc330-2025W1/docs/homework_instructions.html).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-well-done.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "venv (3.13.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  },
  "name": "_merged",
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "438px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
